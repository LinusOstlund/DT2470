{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DT2470_lab2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW87P5Xe8Ixe",
        "colab_type": "text"
      },
      "source": [
        "#DT2470 Lab 02: Synchronization and rhythm\n",
        "\n",
        "by Andre Holzapfel\n",
        "\n",
        "In this lab you will choose to **either** do exercises regarding synchronization (chapter 3), **or** rhythm and tempo processing (chapter 6). We agree that doing both would be too much work (for this year), but you will have the code for the solutions for both anyway!\n",
        "\n",
        "Again, you can use whatever software you want for your own implementation, but I use python in my explanations, and two external libraries in python need to be used in this Lab. Just as Bob, I am basically learning Python by working on this lab, so excuse my bad programming. (I am a bad programmer independent of language, anyway.)\n",
        "\n",
        "Your task is to reproduce all shown figures with your code (or show the equivalent plots for your own audio files, if you use different ones). Also, provide short answers to the questions in the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm0Yelej-P4q",
        "colab_type": "text"
      },
      "source": [
        "##Audio samples:\n",
        "\n",
        "In my examples I use file from the following google folder:\n",
        "\n",
        "https://drive.google.com/drive/folders/1TGj4ZcEgdGXt9Eko2qViiiKWkUDDewbL?usp=sharing\n",
        "\n",
        "You can copy these files to your own google folder and use them as explained in Lab 1.\n",
        "\n",
        "At some points I will ask you to use some audio examples of your own. Be creative...the goal is to check where systems work or fail, and understand (to some extent) those behaviors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2UhoPnSA5Ce",
        "colab_type": "text"
      },
      "source": [
        "## Code preparations\n",
        "\n",
        "As for Lab 1, you need to have some external libraries also here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHAf2h_oQt8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install a python library for working with audio files, named pydub\n",
        "\n",
        "!pip install pydub\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmrtPi6-6ZMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#other preparations!\n",
        "import os, sys\n",
        "from scipy import signal\n",
        "import pydub\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo9zaZZkBRsy",
        "colab_type": "text"
      },
      "source": [
        "### Google drive:\n",
        "\n",
        "you need to adapt the following code to use the audio files that you copied to your google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvOh37d4T7Y-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIzP9-B4CCa8",
        "colab_type": "text"
      },
      "source": [
        "# 1. Synchronisation\n",
        "\n",
        "Complete the tasks in this part, if you want to focus on the synchronisation (but have a look at the rhythm part below before you decide!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZUDQkP6D5FK",
        "colab_type": "text"
      },
      "source": [
        "## Part 1.1 Warming up\n",
        "\n",
        "Choose two music sound files that can be considered as interpretations of the same \"tune\". In my example I used the cmajor_piano.wav and Cmajor_sung.wav, where a C-major scale is performed, once on piano and once sung. For the beginning, choose some excerpts, and not complete pieces, in order to get started.\n",
        "\n",
        "Read the sound files to a numpy array, and plot the two waveforms. (You did that in Lab 1 too).\n",
        "\n",
        "Below the waveform I get for the piano file: ![waveform](https://drive.google.com/uc?id=1nt5lUZ1IWO8nWDx2mTdddz2pI9CuBfTq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiwsBfdKHa4l",
        "colab_type": "text"
      },
      "source": [
        "## Part 1.2 Compute the spectral magnitude\n",
        "\n",
        "For both signals compute a Short-time Fourier transform (STFT), and plot their magnitude spectra (as in equation 3.1. Use the code from Lab 1 as starting point). Note: Since you are focusing on tonal content in the Chroma features, use a long window (about 100ms).\n",
        "\n",
        "The magnitude spectrum for the piano example is shown below. \n",
        "\n",
        "Question: why do we see so few high-frequency components? (Hint: in lab 1 you plotted db magnitude spectra).\n",
        "\n",
        "![](https://drive.google.com/uc?id=1_fEgRgIG0pPzCiNa6Pqz5lDfk-vfGloX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L5J_2osL7_n",
        "colab_type": "text"
      },
      "source": [
        "## Part 1.3 Compute a log-frequency spectrogram\n",
        "\n",
        "Now, transform the linear frequency axis into a logarithmic one (equation 3.4), and plot the resulting log-frequency spectra for both signals.\n",
        "\n",
        "Hint: you need to assign the frequencies of the STFT coefficients to the desired pitch bins of the 12-tone resolution (equations 3.2 and 3.3, and the related text in the book help). Unless you are a good numpy programmer (I am not), you will need some loop in that code...\n",
        "\n",
        "My piano log-f spectrogram looks as shown below.\n",
        "\n",
        "Question: why do the low-frequency regions look so spread-out and smeary?\n",
        "\n",
        "![](https://drive.google.com/uc?id=1pxHK7CVEX438DB4BAASGEiU3Qi-hw3Sc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHMC7X4uOq5u",
        "colab_type": "text"
      },
      "source": [
        "## Compute the chromagram\n",
        "\n",
        "The chromagram is a feature that captures well harmonic and melodic development in a sound file. You will compute these features, to apply a synchronization (DTW) algorithm to them.\n",
        "\n",
        "\n",
        "\n",
        "1.   Compute the chromagram according to equation 3.6\n",
        "\n",
        "![](https://drive.google.com/uc?id=1Hj26wZ2WHufe1DkZqZoFrJ06kS7y8msS)\n",
        "\n",
        "2.   Apply logarithmic compression to the chromagram. This will be your input feature for the DTW (my example uses gamma=100).\n",
        "\n",
        "![](https://drive.google.com/uc?id=1eX8WhSmn6XxI8sIXxfRxdnssSrf2kICT)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyR5UsA50tYZ",
        "colab_type": "text"
      },
      "source": [
        "## Part 1.4 Compute the synchronization\n",
        "\n",
        "To this end, you will use the DTW as provided by the librosa library. You can see how to use it in the following code:\n",
        "\n",
        "https://librosa.github.io/librosa_gallery/auto_examples/plot_music_sync\n",
        "\n",
        "You can use your two computed chromagrams as inputs X and Y to this function, as they are. Compute the synchronization, and plot the path over the obtained cost matrix D. Your code should start somehow like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrLXT9oxGoPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "D, wp = librosa.core.dtw(X=chromagram, Y=chromagram2, metric='cosine')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnG59bv02C7n",
        "colab_type": "text"
      },
      "source": [
        "The plot I obtain when synchronizing the piano and the voice example:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1RRtacUyBpbXQ8QMEgitRdSuLaH61MAGr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QUAgTVx2qsE",
        "colab_type": "text"
      },
      "source": [
        "## Part 1.5 Compute the same alignment using the chroma features from librosa\n",
        "\n",
        "OK, we went by the course book when computing the features above. Now let us use the librosa library, to compare with our implementation.\n",
        "\n",
        "To this end, you can copy/paste and edit the code from the librosa example provided above, and compute the plots!\n",
        "\n",
        "The chroma features look less noisy for me, probably because I used the logarithmic compression with a bad parameter. Note that the book says that all the parameters need to be adapted with care (which some people might call feature engineering):\n",
        "\n",
        "![](https://drive.google.com/uc?id=1lxDS13V7f_azktMmJP5e9NUvQrp2KQ0c)\n",
        "\n",
        "After that the warping path looks as follows for me:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1K5F9XnD6iyDudo1R9V4oCgmCYTqTzju8)\n",
        "\n",
        "Finally, also compute the alignment visualization between the waveforms, as done in the librosa example. Mine looks as follows:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1feMM7D7w4uK3UajklQm849X3mDPtdkRU)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QMG-hUn6YH8",
        "colab_type": "text"
      },
      "source": [
        "## Part 1.6 Last task: conclude, explore\n",
        "\n",
        "\n",
        "\n",
        "1.   Compare the alignment obtained from \"your\" features with those using the librosa features, are they different? If yes, why do you think?\n",
        "2.   Replace the two simple sound files by two complete songs. You can use the Kafene1 and Kafene2 files in the shared folder, or some other song you like more. Compute the alignment, using the librosa features and provide a plot of the alignment curve. Did it work?\n",
        "3. Find two files where the alignment does not work, and explain why. Note: this can be e.g. because the two versions differ in form (e.g. Refrain repeated in one version), tuning, or instrumentation. Also: the chroma features work best for harmonic instruments, and for western-tonal music.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynd3MzFM_tr5",
        "colab_type": "text"
      },
      "source": [
        "# 2. Rhythm and tempo processing\n",
        "\n",
        "In this part, we compute a spectral-based novelty function (often called spectral flux) to obtain onsets, we compute a Fourier tempogram to get tempo estimates, and we compare the beat tracker presented in the book with a recent state-of-the-art beat tracker that uses deep learning (out of the box using available code!).\n",
        "\n",
        "Show your versions of my plots, and provide us with your code, and answer the questions in the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2chq9nmzAZwR",
        "colab_type": "text"
      },
      "source": [
        "## Part 2.1 Onset detection using spectral-based novelty\n",
        "\n",
        "In the examples below, I use the drumloop120_mono.wav to obtain the plots.\n",
        "\n",
        "As a first step, compute a short-time Fourier transform (STFT) magnitude spectrogram, by applying a short window (I use 30ms, and 10ms hop size). Use the code from Lab 1 as starting point to read audio files and compute the STFT.\n",
        "\n",
        "Then apply logarithmic compression (equation 6.5). Then the log-magnitude spectrum looks as follows for me (with gamma=100):\n",
        "\n",
        "![](https://drive.google.com/uc?id=1F3Y3_iHy6tIZq2guIeQXIQIKPaONTzNO)\n",
        "\n",
        "The next step is to compute the difference over time, and to do what is referred to as half-wave rectification in the book (equation 6.6). Plot the obtained matrix, which looks for me as follows:\n",
        "\n",
        "![](https://drive.google.com/uc?id=17LaxEAAliDfaQ72V807iw4y9FkUdV_SK)\n",
        "\n",
        "As you can see, the impulses of the drum onsets are emphasized in this representation. \n",
        "\n",
        "Now you add this matrix along the columns to get a single vector, which looks as the blue line in the plot below.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1-iC2NFpeOx65vsbhiyk7-sWZ1NSljWde)\n",
        "\n",
        "The yellow line shows the local average function (equation 6.7, mu(n)) that we will use for post processing. Hint: in order to obtain the local average function in python you can use the numpy function convolve(), if you google for \"moving average\" and \"np.convolve\" you will find examples.\n",
        "\n",
        "When you subtract mu(n) from the spectral flux vector, and keep only positive values (equation 6.8), you obtain the final spectral novelty function. In addition, normalize the obtained vector so that the maximum is 1. It looks as follows for me (the blue line):\n",
        "\n",
        "![](https://drive.google.com/uc?id=1rCRnhHkc_sR2d-wGIojRbmkOAAJrwmbb)\n",
        "\n",
        "Finally, we want to obtain onsets! We do this by thresholding the obtained function, which means determining all points at which the function is larger than a certain number. The dotted red lines show the onsets that I get with a threshold of 0.5. (Hint: the stem function from the matplotlib does that plotting for you)\n",
        "\n",
        "What is the value you need to set for the threshold to detect all onsets? Try it with another sound file, is the value the same?\n",
        "\n",
        "Do you observe any other problems with the obtained onset detection?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kQ52xpaJ2rd",
        "colab_type": "text"
      },
      "source": [
        "## Part 2.2 Spectral-based novelty by madmom\n",
        "\n",
        "Madmom is a python toolbox that does some state-of-the-art rhythm processing for you. The repository is here (along with some tutorials):\n",
        "\n",
        "https://github.com/CPJKU/madmom\n",
        "\n",
        "In order to get better novelty features, you can use the code in the following tutorial as starting point:\n",
        "\n",
        "https://github.com/CPJKU/madmom_tutorials/blob/master/onset_detection.ipynb\n",
        "\n",
        "You can **run this tutorial as an addition, if you want to understand more** about state-of-the-art signal processing.\n",
        "\n",
        "In order to use madmom in colab, I have to install it as follows, and to restart the runtime...which means this is better done in a separate python notebook. In order to re-import all libraries and create all needed variables again this separate notebook should start as:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4zWO60m4S76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install python library madmom, for onset detection and beat tracking\n",
        "!pip install --force-reinstall madmom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0Weo_y-TQLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pydub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRVDtLD4S5Hy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import os, sys\n",
        "from scipy import signal\n",
        "import pydub\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import madmom\n",
        "from scipy.ndimage.filters import maximum_filter\n",
        "import IPython.display as ipd\n",
        "\n",
        "filename1 = 'cretansyrtos.wav'#put your file\n",
        "root_path = 'gdrive/My Drive/Teaching/DT2470/lab2/labdata/'#put your path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j981IwbVQ5pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMvOoElLRXaO",
        "colab_type": "text"
      },
      "source": [
        "## Part 2.3 Compute a Fourier tempogram\n",
        "\n",
        "We will use the above spectral-based novelty function as an input, to compute a tempogram. From this tempogram, we can get an estimate of the tempo of a piece.\n",
        "\n",
        "Again, produce the figures that I show below. I use the drumloop again, but you can use more interesting music files as well, for instance I provided the \"cretansyrtos.wav\" as an example of some Cretan folk dance.\n",
        "\n",
        "So, as a starting point, you need to compute the spectral novelty function (equation 6.8) for whatever sound you intend to use.\n",
        "\n",
        "The Fourier tempogram then is nothing but the STFT magnitude of the novelty function, with the frequency values mapped to tempo values (equation 6.24). Below you see what I get for a window size of 8 seconds, and a hop size of 0.5 seconds: \n",
        "\n",
        "![](https://drive.google.com/uc?id=1vgbnAiJ_vlGeO44j_syyIlTy_Z6FyiHm)\n",
        "\n",
        "The red line in the above plot is the tempo estimate that I get, when I look for the maximum of the tempogram in each column in a range from 100 to 200 bpm. Implement this tempo estimation, and report also what you obtain when you do not constrain it to a specific tempo range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8rsUC1mGC8r",
        "colab_type": "text"
      },
      "source": [
        "## Part 2.4 Beat Tracking\n",
        "\n",
        "In this part you will use two beat trackers, which are available in python:\n",
        "\n",
        "\n",
        "\n",
        "1.   The librosa beat tracker, which is the dynamic programming approach presented in the book. An example for how to run the librosa beat tracker: https://musicinformationretrieval.com/beat_tracking.html\n",
        "\n",
        "2.   And the madmom beat tracker, which is using deep learning. \n",
        "\n",
        "Apply both beat trackers to audio examples having the following three characteristics:\n",
        "\n",
        "1. stable tempo, strong onsets (for instance, the drum loop example) \n",
        "2. instable tempo, strong onsets (for instance, the cretansyrtos) \n",
        "3. weak energy onsets (for instance, a string quartet or a choir performance) \n",
        "\n",
        "Provide plots for the waveform with the beats overlaid (using vlines or stem). Also listen to the beat tracking results, which you can do with code that looks somewhat as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcfiWNSosdh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import IPython.display as ipd\n",
        "clicks = librosa.clicks(beat_times, sr=sr, length=len(x))\n",
        "ipd.Audio(x + clicks, rate=sr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIh_FErCQsyf",
        "colab_type": "text"
      },
      "source": [
        "For tracking with madmom, your code should look someshat as shown below. Note that in order to run madmom, I need to install it with the --force-install option, so it might be better to do that in a separate python notebook. A nice example that applies both librosa and madmom is here: https://www.analyticsvidhya.com/blog/2018/02/audio-beat-tracking-for-music-information-retrieval/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26Mulcg_ttyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import madmom\n",
        "proc = madmom.features.beats.DBNBeatTrackingProcessor(fps=100)\n",
        "act = madmom.features.beats.RNNBeatProcessor()(os.path.join(root_path,filename1))\n",
        "beat_times = proc(act)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03z15rTNRTBv",
        "colab_type": "text"
      },
      "source": [
        "Report what differences you observe (mainly by listening to the audio with the clicks for the beats) between the beat tracking outputs obtained from the two algorithms. Why do you think these differences emerge? (Given what you know from the book chapter and the from example on the analyticsvidhya website)\n",
        "\n",
        "The plot of waveform and beats I get for the cretansyrtos example using librosa:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1l1YtJjrCHzaGUDEQ9O4UBafarf983CvW)\n",
        "\n",
        "The plot of waveform and beats I get for the cretansyrtos example using madmom:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1L5LHO2EwgHgg8ypuj57gaj2j0eboCa5k)"
      ]
    }
  ]
}